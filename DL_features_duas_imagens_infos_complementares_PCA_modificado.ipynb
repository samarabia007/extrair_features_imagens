{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1> Extração de features de duas imagens com informações complementares e classificação por CNN </h1>","metadata":{},"id":"7e1a57bc-2672-4fe5-8f70-beb77fd83d5b"},{"cell_type":"markdown","source":"Primeiramente fazemos o alinhamento das imagens que foram obtidas por diferentes técnicas de imageamento. \nNeste exemplo utilizaram-se as obtidas por UV e IR.\nEste é o código para alinhar as duas imagens por pontos-chaves (ORB):","metadata":{},"id":"6c3ea19c-05e6-46fa-b688-d9f7f9502908"},{"cell_type":"code","source":"import cv2 as cv\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\n\n# ler duas imagens\nimg1 = cv.imread('uv20161004rosael-8_reduzida_para_257.jpg', cv.IMREAD_GRAYSCALE)\nimg2 = cv.imread('quiteriaIR_reduzida_para_257.jpg', cv.IMREAD_GRAYSCALE)\n\n# Dar o ORB nelas\norb = cv.ORB_create()\nkp1, des1 = orb.detectAndCompute(img1, None)\nkp2, des2 = orb.detectAndCompute(img2, None)\n\n# Dar o Brute Force Matcher e os descriptors bf.Match(des1, des2)\nbf = cv.BFMatcher (cv.NORM_HAMMING, crossCheck = True)\nmatches = bf.match(des1, des2)\n\n# Ordena-os por distância\nmatches = sorted(matches, key = lambda x:x.distance)\n\n# Cria variáveis com uma das dimensões do tamanho do número de matches \nmatches = matches[:int(len(matches)*90)]\nno_of_matches = len(matches)\np1 = np.zeros((no_of_matches, 2))\np2 = np.zeros((no_of_matches, 2))\n\n# Atribui o queryIdx dos kp1 dos matches da img1 à variável p1 e os trainIdx da img2 (à variável p2)\nfor i in range(len(matches)):\n    p1[i, :] = kp1[matches[i].queryIdx].pt\n    p2[i, :] = kp2[matches[i].trainIdx].pt\n    \n# Cria máscara, homografia e imagem transformada por elas\nhomography, mask = cv.findHomography(p1, p2, cv.RANSAC)\ntransformed_img = cv.warpPerspective(img1, homography, (706,705))\ncv.imwrite('imagem_transformada.jpg', transformed_img)\n                                     \n# Desenha os 10 primeiros matches\nimg3 = cv.drawMatches(img1,kp1,img2,kp2,matches[:10],None,flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\nplt.imshow(img3),plt.show()","metadata":{},"execution_count":null,"outputs":[],"id":"65611aef-5eb0-4fcb-b50d-af6ee478fcc5"},{"cell_type":"markdown","source":"Após alinhadas as imagens, sabemos que os pixelsets de uma correspondem à mesma região de pintura dos pixelsets da outra. Assim, podemos extrair informações de cada pixelset das duas imagens somadas, ou melhor, de trechos de um array que é a soma das duas obtido através de:","metadata":{},"id":"0d87d2c6-a18c-4e01-b878-903d05a8f00f"},{"cell_type":"code","source":"import cv2 as cv\nimport numpy as np\n\npath = 'original'\n\nimg1 = cv.imread (path + '/' + 'MariaQuiteria-DomenicoFailutti-MP-USP-RAD-WEB-PedroCampos-MarciaRizzutto-IFUSP-2_reduzida_para_257_margem.jpg')\nimg1array = np.array(img1)\nimg2 = cv.imread (path + '/' + 'quiteriaIR_reduzida_para_257.jpg')\nimg2array = np.array(img2)\n\nimgs_somadas = ((257,257,6))\n\nimgs_somadas [:,:,0:3] = img1array\nimgs_somadas [:,:,3:] = img2array\n\nnp.save('imgs_somadas.npy',imgs_somadas)","metadata":{},"execution_count":null,"outputs":[],"id":"91708df4-2c1b-42e5-88ca-b7eb50914b29"},{"cell_type":"markdown","source":"Para extrair as features dos pixelsets, vamos recortar o array em que já estão as infos somadas de cada imagem. Os pixelsets serão os dados que informaremos à CNN para ela aprender quais se tratam de intervenção e quais não, conforme a pasta de onde eles são lidos (conforme os labels):","metadata":{},"id":"2630798a-c7f9-4207-8b62-a7bccbec96df"},{"cell_type":"code","source":"import numpy as np\nimport cv2 as cv\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport seaborn as sns\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Dense\nfrom sklearn.model_selection import train_test_split\n\ndef treina_modelo(modelo, num_classes):\n    model.add (Conv2D(32,(3,3),\n                #activation = 'relu',\n                input_shape = (5,5,3)))\n                #padding = 'same'))\n    model.add(MaxPooling2D(pool_size=(32,32),\n                           padding='same'))\n    model.add(Dense(128,activation='relu'))\n    model.add(Dense(2,activation='softmax'))\n    print ('rodando treinamento do modelo')\n    print (type(model))\n    model.compile(loss=keras.losses.mean_squared_error,\n                  optimizer = 'adam',\n                  metrics = ['accuracy'])\n    return model\n\ndef le_imagens(path):\n    x = []\n    y = []\n\n    labels = os.listdir(path)\n    print('-'*10)\n    print(labels)\n    print('-'*10)\n    \n    for label in labels:\n        print('Loading: ',label)\n        files = os.listdir(path + '/' + label)\n        print('-'*10)\n        print(files)\n        print('-'*10)\n        for file in tqdm(files):\n            img = cv.imread (path + '/' + label + '/' + file)\n            imgarray = np.array(img)\n            x.append(imgarray)\n            y.append(label)\n    x = np.array(x)\n    y = np.array(y)\n    y = pd.get_dummies(y).values\n    \n    return x, y, img\n\nif __name__==\"__main__\":\n    path = r'imagens_recortadas1'\n    num_classes = 2\n    epochs = 10\n    batch_size = 36\n    x, y, imagem = le_imagens(path) \n    print (type(x))\n    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.33, random_state = 23)\n    x_test, x_val, y_test, y_val = train_test_split(x_val, y_val, test_size = 0.33, random_state = 23)\n    \n    model = tf.keras.Sequential()\n\n    model = treina_modelo(model, num_classes)\n\n    model.fit(x_train, y_train,\n        batch_size=batch_size,\n        epochs=epochs,\n        verbose=1,\n        validation_data=(x_val, y_val))\n    score = model.evaluate(x_test, y_test, verbose=0)\n    print('Test loss:', score[0])\n    print('Test accuracy:', score[1])\n\n    model.save(\"portrait_net_model.h5\")\n    model.summary()","metadata":{},"execution_count":null,"outputs":[],"id":"dfcf1f0c-b490-4ed6-ab63-4bd17344fcbc"},{"cell_type":"markdown","source":"Após aplicar essas convoluções, procede-se à aplicação da Principal Component Analysis (PCA) para observar como as informações estão: ->>> É aqui que preciso de ajuda! <<<-","metadata":{},"id":"c1251ff8-3566-4418-ac14-525e60c0797b"},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\ndf = pd.DataFrame(dados_saida_do_modelo)\nX = df.drop('target',1) # corrigir para retirar a ultima coluna, simplesmente\n    y = df['target'] # corrigir para pegar a ultima coluna, não 'target'\n    pca = PCA(n_components=2)\n    pca.fit(X)\n\n    mean_vec = np.mean(X, axis=0)\n    M = X - mean_vec\n    C = M.T.dot(M) / (X.shape[0]-1)\n    autovalores, autovetores = np.linalg.eig(C)\n    print (autovalores)\n    \n    pares_de_autos = [\n        (\n                np.abs(autovalores[i]),\n                autovetores[:,i]\n        ) for i in range(len(autovalores)) \n    ]\n    pares_de_autos.reverse()\n    \n   #graficando\n    n_componentes = 2\n    autovetores = [p[1] for p in pares_de_autos]\n    A = autovetores[0:n_componentes]\n    X = np.dot(X,np.array(A).T)\n    \n    new_df = pd.DataFrame(X,columns=['pc1','pc2'])\n    new_df['target']=df['target']\n    sns.pairplot(\n    \tnew_df, vars = ['pc1','pc2'], hue='target', diag_kind=\"hist\"\n    )\n    plt.show() ","metadata":{},"execution_count":null,"outputs":[],"id":"580a0f77-351a-4c07-a2f1-9fe7f98e7178"},{"cell_type":"markdown","source":"Para extrair os dados a serem utilizados na PCA, ao invés de utilizar o modelo Sequential do código acima, também experimentou-se a seguinte extração por Transfer Learning da VGG16:","metadata":{},"id":"ffc68257-3791-47ad-bcf4-945e421792fc"},{"cell_type":"markdown","source":"(a modificar ainda, para ler a soma das imagens, não somente a de Raio X)","metadata":{},"id":"db759b5e-60ac-4c04-b71e-bad86a567c87"},{"cell_type":"code","source":"from tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nimport numpy as np\n\nmodel = VGG16(weights='imagenet', include_top=False)\n\nx = []\n\nimg_path = 'original/MariaQuiteria-DomenicoFailutti-MP-USP-RAD-WEB-PedroCampos-MarciaRizzutto-IFUSP-2_reduzida_para_257.jpg'\nimg = image.load_img(img_path,target_size=(224,224))            \nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\nfeatures = model.predict(x)\nprint(features.shape)\nprint(features)","metadata":{},"execution_count":null,"outputs":[],"id":"d045340e-a755-4304-9572-32e1923a973a"},{"cell_type":"code","source":"(inserir resultado da PCA)","metadata":{},"execution_count":null,"outputs":[],"id":"ebccd1e6-2783-49ce-96fd-67f419d94179"},{"cell_type":"markdown","source":"Uma alternativa para esse processamento acima é o Autoencoder, em que não utilizaríamos os labels \"original\" ou \"com_intervencao\" para treinar o sistema, mas sim realizaríamos um aprendizado não-supervisionado:","metadata":{},"id":"cdb66975-90e1-491c-ac5a-0f9ae53d5319"},{"cell_type":"code","source":"import os\nimport cv2 as cv\nimport numpy as np\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten, Input\nfrom keras.layers import Conv2D, MaxPooling2D, UpSampling2D\nfrom sklearn.tree import DecisionTreeClassifier\n\n# modela o codificador e o decodificador\ndef build_model(x,y):\n    \n    #1a camada de convolução\n    model.add(Conv2D(16, (3, 3), padding='same', input_shape=(224,224,3)))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n    \n    #2a camada de convolução\n    model.add(Conv2D(2,(3, 3), padding='same'))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n    #-------------------------\n    #3a camada de convolução\n    model.add(Conv2D(2,(3, 3), padding='same'))\n    model.add(Activation('relu'))\n    model.add(UpSampling2D((2, 2)))\n    \n    #4a camada de convolução\n    model.add(Conv2D(16,(3, 3), padding='same'))\n    model.add(Activation('relu'))\n    model.add(UpSampling2D((2, 2)))\n    \n    #-------------------------\n    \n    model.add(Conv2D(3,(3, 3), padding='same'))\n    model.add(Activation('sigmoid'))\n    \n    model.summary()\n    \n    # Compila o modelo\n    model.compile(optimizer='adadelta', loss='binary_crossentropy')\n    \n    return model\n\n# le dados\ndef read_data(path):\n    x = []\n \n    files = os.listdir(path)\n    for file in tqdm(files):\n        img = cv.imread (path + '/' + file)\n        imgarray = np.array(img)\n        x.append(imgarray)\n       \n    x = np.array(x)\n    \n    return x\n\n# main\nif __name__==\"__main__\":\n    \n    path = r'array_selecionado_para_autoencoder'\n    x = read_data(path)\n    y = x[0] # trocar por um Flatten do x\n\n    batch_size = 32\n    epochs = 60\n    model = Sequential()\n    model = build_model(x,y) #ver se esta linha está funcionando mesmo\n\n    model.save(\"autoencoder_model.h5\")\n\n# ------>>>>>>>>>>>>>> Árvore de decisão:\n    clf = DecisionTreeClassifier(model)","metadata":{},"execution_count":null,"outputs":[],"id":"624b9545-df15-4d29-8e22-83f9e9481308"},{"cell_type":"markdown","source":"Após extrair as features, pretende-se aplicar ao modelo uma camada de Árvore de Decisão, para dividir em duas \nclasses os recortes que foram processados: Original e Com intervenção.","metadata":{},"id":"a15e3ee7-ff26-42ae-90c6-fe62eee7aacb"},{"cell_type":"raw","source":"Resultados das features extraídas pelo Autoencoder(?)\nO Summary do modelo eu consegui pegar (está a seguir), mas como pegar as features? ->>> É aqui que preciso de ajuda! <<<-\n\nLayer (type)                Output Shape              Param #   \n=================================================================\n conv2d_10 (Conv2D)          (None, 224, 224, 16)      448       \n                                                                 \n activation_10 (Activation)  (None, 224, 224, 16)      0         \n                                                                 \n max_pooling2d_4 (MaxPooling  (None, 112, 112, 16)     0         \n 2D)                                                             \n                                                                 \n conv2d_11 (Conv2D)          (None, 112, 112, 2)       290       \n                                                                 \n activation_11 (Activation)  (None, 112, 112, 2)       0         \n                                                                 \n max_pooling2d_5 (MaxPooling  (None, 56, 56, 2)        0         \n 2D)                                                             \n                                                                 \n conv2d_12 (Conv2D)          (None, 56, 56, 2)         38        \n                                                                 \n activation_12 (Activation)  (None, 56, 56, 2)         0         \n                                                                 \n up_sampling2d_4 (UpSampling  (None, 112, 112, 2)      0         \n 2D)                                                             \n                                                                 \n conv2d_13 (Conv2D)          (None, 112, 112, 16)      304       \n                                                                 \n activation_13 (Activation)  (None, 112, 112, 16)      0         \n                                                                 \n up_sampling2d_5 (UpSampling  (None, 224, 224, 16)     0         \n 2D)                                                             \n                                                                 \n conv2d_14 (Conv2D)          (None, 224, 224, 3)       435       \n                                                                 \n activation_14 (Activation)  (None, 224, 224, 3)       0         \n                                                                 \n=================================================================\nTotal params: 1,515\nTrainable params: 1,515\nNon-trainable params: 0","metadata":{},"id":"4e3e37c8-f381-4ba4-96e3-4edc958b01bf"}]}
